{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries in this cell\n",
    "import pandas as pd #pandas is a library for data wrangling/handling\n",
    "import numpy as np #same case for numpy\n",
    "\n",
    "# Libraries for helping us with strings\n",
    "import string\n",
    "# Regular Expression Library\n",
    "import re\n",
    "\n",
    "# Seaborn / matplotlib for visualization \n",
    "import seaborn as sns\n",
    "# This command tells python to use seaborn for its styling.\n",
    "sns.set()\n",
    "\n",
    "\n",
    "# Matplotlib is also a very useful, basic visualization/plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "# Very important, this will make your charts appear in your notebook instead of in a new window.\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Provides z-score helper function,\n",
    "# z-score uses standard deviation to remove outliers\n",
    "# (industry standard is if a data point is 3 std devs away from mean,\n",
    "# it's considered to be an outlier)\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# Ignore this, this is just for displaying images.\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# Importing sklearn library\n",
    "import sklearn\n",
    "\n",
    "# Import the trees from sklearn\n",
    "from sklearn import tree\n",
    "\n",
    "# Metrics help us score our model, using metrics to evaluate our model\n",
    "from sklearn import metrics\n",
    "\n",
    "# Import our Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import our Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Import our text vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# This is our Logit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Importing our linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Helper fuctions to evaluate our model from sklearn, including f1_score.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "# Some more helpful ML function\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Helper function to split our data for testing and training purposes\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Helper function for hyper-parameter turning.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import MultinomaialNB classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Import our Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Library for visualizing our tree\n",
    "# If you get an error, \n",
    "# run 'conda install python-graphviz' in your terminal (without the quotes).\n",
    "import graphviz \n",
    "\n",
    "\n",
    "# NLTK is our Natural-Language-Took-Kit\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# You may need to download these from nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns in dataframe: (37249, 2) \n",
      "\n",
      "                                       clean_comment  category\n",
      "0   family mormon have never tried explain them t...         1\n",
      "1  buddhism has very much lot compatible with chr...         1\n",
      "2  seriously don say thing first all they won get...        -1\n",
      "3  what you have learned yours and only yours wha...         0\n",
      "4  for your own benefit you may want read living ...         1\n",
      "******************** \n",
      "Is null:\n",
      "clean_comment    100\n",
      "category           0\n",
      "dtype: int64 \n",
      "\n",
      "% of null and dup in data\n",
      "clean_comment    0.27\n",
      "category         0.00\n",
      "dtype: float64\n",
      "Number of dupes are 350 \n",
      "\n",
      "total number of dupes:  0\n",
      "\n",
      "Number of rows after cleaning data:  36799\n",
      "[ 1 -1  0]\n"
     ]
    }
   ],
   "source": [
    "#First dataframe\n",
    "df_reddit_data = pd.read_csv('data/Reddit_Data.csv')\n",
    "print(\"Number of rows and columns in dataframe: \" + str(df_reddit_data.shape), '\\n')\n",
    "print(df_reddit_data.head())\n",
    "\n",
    "print(\"*\"*20, \"\\nIs null:\")\n",
    "print(df_reddit_data.isnull().sum(), '\\n')\n",
    "print(\"% of null and dup in data\")\n",
    "print(((df_reddit_data.isnull().sum() / len(df_reddit_data)) *100).round(2))\n",
    "# Dropping nulls\n",
    "df_reddit_data.dropna(inplace=True)\n",
    "\n",
    "# Checking for duplicates\n",
    "print(\"Number of dupes are\", df_reddit_data.duplicated().sum(), \"\\n\")\n",
    "\n",
    "#Dropping dupes\n",
    "df_reddit_data.drop_duplicates(inplace=True)\n",
    "\n",
    "#checking dupes\n",
    "print(\"total number of dupes: \",df_reddit_data.duplicated().sum())\n",
    "\n",
    "# total number of rows:\n",
    "print('\\nNumber of rows after cleaning data: ', df_reddit_data.shape[0])\n",
    "\n",
    "print(df_reddit_data.category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['clean_comment', 'category'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for the type of columns, to see if some are faulty\n",
    "# including duplicated columns or faulty ones that don't have an name for example\n",
    "df_reddit_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns in dataframe: (162980, 2) \n",
      "\n",
      "                                       clean_comment  category\n",
      "0  when modi promised â€œminimum government maximum...      -1.0\n",
      "1  talk all the nonsense and continue all the dra...       0.0\n",
      "2  what did just say vote for modi  welcome bjp t...       1.0\n",
      "3  asking his supporters prefix chowkidar their n...       1.0\n",
      "4  answer who among these the most powerful world...       1.0\n",
      "******************** \n",
      "Is null:\n",
      "clean_comment    4\n",
      "category         7\n",
      "dtype: int64 \n",
      "\n",
      "% of null and dup in data\n",
      "clean_comment    0.0\n",
      "category         0.0\n",
      "dtype: float64\n",
      "Number of dupes are 0 \n",
      "\n",
      "total number of dupes:  0\n",
      "\n",
      "Number of rows after cleaning data:  162969\n",
      "[-1.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "#Second dataframe\n",
    "df_twitter_data = pd.read_csv('data/Twitter_Data.csv')\n",
    "df_twitter_data = df_twitter_data.rename(columns = {'clean_text' : 'clean_comment'})\n",
    "print(\"Number of rows and columns in dataframe: \" + str(df_twitter_data.shape), '\\n')\n",
    "print(df_twitter_data.head())\n",
    "\n",
    "print(\"*\"*20, \"\\nIs null:\")\n",
    "print(df_twitter_data.isnull().sum(), '\\n')\n",
    "print(\"% of null and dup in data\")\n",
    "print(((df_twitter_data.isnull().sum() / len(df_twitter_data)) *100).round(2))\n",
    "# Dropping nulls\n",
    "df_twitter_data.dropna(inplace=True)\n",
    "\n",
    "# Checking for duplicates\n",
    "print(\"Number of dupes are\", df_twitter_data.duplicated().sum(), \"\\n\")\n",
    "\n",
    "#Dropping dupes\n",
    "df_twitter_data.drop_duplicates(inplace=True)\n",
    "\n",
    "#checking dupes\n",
    "print(\"total number of dupes: \",df_twitter_data.duplicated().sum())\n",
    "\n",
    "# total number of rows:\n",
    "print('\\nNumber of rows after cleaning data: ', df_twitter_data.shape[0])\n",
    "\n",
    "print(df_twitter_data.category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>clean_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                      clean_comment\n",
       "0         0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1         0  is upset that he can't update his Facebook by ...\n",
       "2         0  @Kenichan I dived many times for the ball. Man...\n",
       "3         0    my whole body feels itchy and like its on fire \n",
       "4         0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new data from sentiment tweets:\n",
    "# temo is abreviation of twitter emotion\n",
    "df_temo = pd.read_csv('data/twitter_emotion.csv',  encoding='latin-1', names=[\"emotion\",\"sp_id\", \"date\", \"query\", \"user\", \"message\"])\n",
    "df_temo = df_temo.drop(columns = ['date', 'query', \"user\", \"sp_id\"])\n",
    "df_temo = df_temo.rename(columns = {'message' : 'clean_comment', 'emotion' : 'category'})\n",
    "df_temo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category          int64\n",
      "clean_comment    object\n",
      "dtype: object\n",
      "   category                                      clean_comment\n",
      "0         0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1         0  is upset that he can't update his Facebook by ...\n",
      "2         0  @Kenichan I dived many times for the ball. Man...\n",
      "3         0    my whole body feels itchy and like its on fire \n",
      "4         0  @nationwideclass no, it's not behaving at all....\n",
      "[0 1]\n",
      "Number of dupes are 16309\n",
      "Number of dupes are 0\n"
     ]
    }
   ],
   "source": [
    "#mask1 = df[\"category\"] == -1\n",
    "\n",
    "#df.loc[mask1, 'category'] = 0\n",
    "#print(df[\"category\"].unique())\n",
    "\n",
    "#csv_file_name = \"data/cleaned_agg_data.csv\"\n",
    "\n",
    "#df.to_csv(csv_file_name, index=False)\n",
    "#df_temo[\"message\"] = df_temo[\"message\"].str.decode(\"utf-8\")\n",
    "\n",
    "# bytes(row[\"message\"], 'utf-8').decode('utf-8', 'ignore')\n",
    "\n",
    "def vec_func(arg_df):   \n",
    "    return pd.Series( [bytes(sentence, 'utf-8').decode('utf-8', 'ignore') for sentence in arg_df[\"clean_comment\"]])\n",
    "\n",
    "\n",
    "df_temo[\"clean_comment\"] = vec_func(df_temo)\n",
    "print(df_temo.dtypes)\n",
    "\n",
    "print(df_temo.head())\n",
    "mask_1 = df_temo[\"category\"] < 3 \n",
    "mask_2 = ~mask_1 # the opposite of mask 1\n",
    "df_temo.loc[mask_1, 'category'] = 0\n",
    "df_temo.loc[mask_2, 'category'] = 1\n",
    "print(df_temo[\"category\"].unique())\n",
    "\n",
    "df_temo.dropna(inplace=True)\n",
    "# Checking for duplicates\n",
    "print(\"Number of dupes are \" + str(df_temo.duplicated().sum()))\n",
    "\n",
    "#Dropping dupes\n",
    "df_temo.drop_duplicates(inplace=True)\n",
    "print(\"Number of dupes are \" + str(df_temo.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  clean_comment  category\n",
      "0     americunt        -1\n",
      "1      as_hell         -1\n",
      "2          ass         -1\n",
      "3       asshole        -1\n",
      "4       bastard        -1\n",
      "clean_comment    0\n",
      "category         0\n",
      "dtype: int64 \n",
      "\n",
      "clean_comment    0.0\n",
      "category         0.0\n",
      "dtype: float64\n",
      "Number of dupes are 41\n",
      "0\n",
      "Number of nulls: clean_comment    0\n",
      "category         0\n",
      "dtype: int64\n",
      "\n",
      "Number of dupes are 0\n",
      "\n",
      "Number of rows after cleaning data:  1676\n",
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "#Fourth dataframe\n",
    "# change underscores to hypens !!important\n",
    "df_more_bad_words = pd.read_csv('data/more_bad_words.csv', names=[\"clean_comment\"])\n",
    "df_bad_words = pd.read_csv(\"data/bad_words.csv\", names=[\"clean_comment\"])\n",
    "\n",
    "df_profanity = pd.concat([df_more_bad_words, df_bad_words], ignore_index=True)\n",
    "df_profanity[\"category\"] = -1\n",
    "\n",
    "print(df_profanity.head())\n",
    "\n",
    "# Checking for nulls\n",
    "print(df_profanity.isnull().sum(), '\\n')\n",
    "print(((df_profanity.isnull().sum() / len(df_profanity)) *100).round(2))\n",
    "df_profanity.dropna(inplace=True)\n",
    "\n",
    "# Checking for duplicates\n",
    "print(\"Number of dupes are \" + str(df_profanity.duplicated().sum()))\n",
    "\n",
    "#Dropping dupes\n",
    "\n",
    "df_profanity.drop_duplicates(inplace=True)\n",
    "print(df_profanity.duplicated().sum())\n",
    "# Sanity Checking\n",
    "print('Number of nulls: ' + str(df_profanity.isnull().sum()))\n",
    "print(\"\\nNumber of dupes are \" + str(df_profanity.duplicated().sum()))\n",
    "# print(str(df_reddit_data.duplicated()[condition]))\n",
    "print('\\nNumber of rows after cleaning data: ', df_profanity.shape[0])\n",
    "print(df_profanity.category.unique())\n",
    "\n",
    "#Saving the profanity separately as well, just in case\n",
    "filename = 'pkl_files/profanity.pkl'\n",
    "pickle.dump(df_profanity, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human' 'bot']\n",
      "[1 -1]\n"
     ]
    }
   ],
   "source": [
    "# I ran this cell to merge the deepfake dataset with the reddit and twitter datasets\n",
    "\n",
    "# merging the deep fake datasets\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_valid = pd.read_csv(\"data/validation.csv\")\n",
    "\n",
    "df_test = df_test.drop(columns = ['screen_name', 'class_type'])\n",
    "df_train = df_train.drop(columns = ['screen_name', 'class_type'])\n",
    "df_valid = df_valid.drop(columns = ['screen_name', 'class_type'])\n",
    "\n",
    "# have to run this cell again once i change the values in the second column\n",
    "df_list = [df_test, df_train, df_valid]\n",
    "# pd.concat() has a parameter (index_ignore) that will rid us of the problem a useless index\n",
    "df_deepfake = pd.concat(df_list, ignore_index = True)\n",
    "\n",
    "df_deepfake = df_deepfake.rename(columns = {'text' : 'clean_comment', 'account.type' : 'category'})\n",
    "\n",
    "\n",
    "print(df_deepfake[\"category\"].unique())\n",
    "condition1 = df_deepfake['category'] == 'human'\n",
    "condition2 = df_deepfake['category'] == 'bot'\n",
    "\n",
    "df_deepfake.loc[condition1, 'category'] = 1\n",
    "df_deepfake.loc[condition2, 'category'] = -1\n",
    "print(df_deepfake[\"category\"].unique())\n",
    "\n",
    "\n",
    "#saved this as a separate dataframe here using pickle, just in case\n",
    "filename = 'pkl_files/deepfake.pkl'\n",
    "\n",
    "pickle.dump(df_deepfake, open(filename, \"wb\"))\n",
    "\n",
    "df_dpfk = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My attempt at merging the reddit and twitter datasets together\n",
    "list_of_df = [df_reddit_data, df_twitter_data, df_profanity, df_dpfk, df_temo]\n",
    "\n",
    "df_posts = pd.concat(list_of_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1810707\n",
      "(1810707, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       clean_comment category\n",
       "0   family mormon have never tried explain them t...        1\n",
       "1  buddhism has very much lot compatible with chr...        1\n",
       "2  seriously don say thing first all they won get...       -1\n",
       "3  what you have learned yours and only yours wha...        0\n",
       "4  for your own benefit you may want read living ...        1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(len(a) for a in list_of_df))\n",
    "print(str(df_posts.shape))\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dupes are 26059 \n",
      "\n",
      "(1784648, 2)\n",
      "Index(['clean_comment', 'category'], dtype='object')\n",
      "[1 -1 0]\n"
     ]
    }
   ],
   "source": [
    "# Just some more sanity checking\n",
    "# Dropping nulls\n",
    "\n",
    "#replace urls\n",
    "df_posts[\"clean_comment\"] = df_posts[\"clean_comment\"].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "# replace @mentions in twitter and other platforms\n",
    "df_posts[\"clean_comment\"] = df_posts[\"clean_comment\"].str.replace('@\\S+', '', case=False)\n",
    "\n",
    "df_posts.dropna(inplace=True)\n",
    "# Checking for duplicates\n",
    "print(\"Number of dupes are\", df_posts.duplicated().sum(), \"\\n\")\n",
    "#Dropping dupes\n",
    "df_posts.drop_duplicates(inplace=True)\n",
    "\n",
    "print(df_posts.shape)\n",
    "print(df_posts.columns)\n",
    "print(df_posts.category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1676, 2)\n",
      "1676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>americunt</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as_hell</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ass</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asshole</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bastard</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clean_comment  category\n",
       "0     americunt        -1\n",
       "1      as_hell         -1\n",
       "2          ass         -1\n",
       "3       asshole        -1\n",
       "4       bastard        -1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(str(df_profanity.shape))\n",
    "print(len(df_profanity[\"clean_comment\"].unique()))\n",
    "df_profanity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of profane words that shouldn't be allowed\n",
    "profanity_list = set(df_profanity[\"clean_comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filer words out using profanity from df_bad_words\n",
    "def remove_profanity(profane_str):\n",
    "    words = word_tokenize(profane_str)\n",
    "    valid_words = []\n",
    "    for word in words:\n",
    "        if word not in profanity_list:\n",
    "            valid_words.append(word)\n",
    "    profane_str = ' '.join(valid_words)\n",
    "    return profane_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seems like good practice to me, creating a single function that will call all\n",
    "# our necessary functions from one place, will be subject to change\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def text_pipeline(input_str):\n",
    "    input_str = remove_profanity(input_str)\n",
    "    return input_str\n",
    "\n",
    "def mk_lower(a):\n",
    "    return a.lower()\n",
    "\n",
    "def remove_stopwords(a):\n",
    "    return \" \".join([word for word in word_tokenize(a) if word not in stopwords])\n",
    "\n",
    "def remove_sp_char(a):\n",
    "    ## \\s for white space, ^ is negation, \\w is words.  so replace all punctutation that follows a word \n",
    "    return a.translate(translator)\n",
    "\n",
    "def remove_sp_char2(a):\n",
    "    # fill in first quote\n",
    "    #return a.str.replace(\"\", \"\", case=False)\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", a)\n",
    "\n",
    "def text_pipeline2(a):\n",
    "    a = mk_lower(a)\n",
    "    #a = remove_mentions(a)\n",
    "    #a = remove_urls(a)\n",
    "    a = remove_sp_char(a)\n",
    "    a = remove_stopwords(a)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_posts[\"clean_comment2\"] = df_posts['clean_comment']\n",
    "\n",
    "# applying pipeline to cutdown the size of the file\n",
    "df_posts[\"clean_comment\"] = df_posts['clean_comment'].apply(text_pipeline2)\n",
    "\n",
    "#df_posts['clean_comment_profane_free'] = df_posts['clean_comment2'].apply(text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             clean_comment category\n",
      "0        family mormon never tried explain still stare ...        1\n",
      "1        buddhism much lot compatible christianity espe...        1\n",
      "2        seriously say thing first get complex explain ...       -1\n",
      "3        learned want teach different focus goal wrappi...        0\n",
      "4        benefit may want read living buddha living chr...        1\n",
      "...                                                    ...      ...\n",
      "1810702                      woke school best feeling ever        1\n",
      "1810703       thewdbcom cool hear old walt interviews Ã¢Â™ Â«        1\n",
      "1810704                    ready mojo makeover ask details        1\n",
      "1810705  happy 38th birthday boo alll time tupac amaru ...        1\n",
      "1810706                               happy charitytuesday        1\n",
      "\n",
      "[1784648 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# clean_comment is the original value of everything, unfiltered\n",
    "# clean_comment2 is the clean_comment column but filtered through a pipeline of functions that filter the text\n",
    "# clean_comment_profane_free is the clean_comment2 column applied with an additional (profanity) filter\n",
    "print(df_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME YOUR MODEL \n",
    "filename = 'pkl_files/comments.pkl'\n",
    "\n",
    "# EXPORT AND SAVE df\n",
    "pickle.dump(df_posts, open(filename, \"wb\"))\n",
    "\n",
    "## HOW TO LOAD IT FOR FUTURE USE\n",
    "df = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             clean_comment category\n",
      "0        family mormon never tried explain still stare ...        1\n",
      "1        buddhism much lot compatible christianity espe...        1\n",
      "2        seriously say thing first get complex explain ...       -1\n",
      "3        learned want teach different focus goal wrappi...        0\n",
      "4        benefit may want read living buddha living chr...        1\n",
      "...                                                    ...      ...\n",
      "1810702                      woke school best feeling ever        1\n",
      "1810703       thewdbcom cool hear old walt interviews Ã¢Â™ Â«        1\n",
      "1810704                    ready mojo makeover ask details        1\n",
      "1810705  happy 38th birthday boo alll time tupac amaru ...        1\n",
      "1810706                               happy charitytuesday        1\n",
      "\n",
      "[1784648 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "## run this cell if you want to export the data as a raw .csv file\n",
    "mask1 = df[\"category\"] == -1\n",
    "\n",
    "df.loc[mask1, 'category'] = 0\n",
    "print(df[\"category\"].unique())\n",
    "\n",
    "csv_file_name = \"data/cleaned_agg_data.csv\"\n",
    "\n",
    "df.to_csv(csv_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
